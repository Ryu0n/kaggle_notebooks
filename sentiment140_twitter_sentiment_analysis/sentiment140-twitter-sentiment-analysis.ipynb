{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis\n1. Text input\n2. Stemming / Lemmatization\n3. Tokenization\n4. Classification\n5. Stopword filtering\n6. Sentiment class\n7. Negation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport re\n\nprint(\"Tensorflow Version : \", tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:06:37.673477Z","iopub.execute_input":"2022-02-05T12:06:37.673795Z","iopub.status.idle":"2022-02-05T12:06:42.683834Z","shell.execute_reply.started":"2022-02-05T12:06:37.673715Z","shell.execute_reply":"2022-02-05T12:06:42.683075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', encoding='latin', header=None)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:06:49.233687Z","iopub.execute_input":"2022-02-05T12:06:49.233959Z","iopub.status.idle":"2022-02-05T12:06:54.846608Z","shell.execute_reply.started":"2022-02-05T12:06:49.23393Z","shell.execute_reply":"2022-02-05T12:06:54.845363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:07:45.45263Z","iopub.execute_input":"2022-02-05T12:07:45.452934Z","iopub.status.idle":"2022-02-05T12:07:45.474968Z","shell.execute_reply.started":"2022-02-05T12:07:45.4529Z","shell.execute_reply":"2022-02-05T12:07:45.474059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:08:14.742817Z","iopub.execute_input":"2022-02-05T12:08:14.743096Z","iopub.status.idle":"2022-02-05T12:08:14.748921Z","shell.execute_reply.started":"2022-02-05T12:08:14.743066Z","shell.execute_reply":"2022-02-05T12:08:14.747892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need only 'text' and 'sentiment' columns to sentiment analysis.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:07:59.302411Z","iopub.execute_input":"2022-02-05T12:07:59.303108Z","iopub.status.idle":"2022-02-05T12:07:59.33723Z","shell.execute_reply.started":"2022-02-05T12:07:59.303071Z","shell.execute_reply":"2022-02-05T12:07:59.336536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_to_sentiment = {0: 'Negative', 4: 'Positive'}\n\ndef decode_label(label):\n    return lab_to_sentiment[label]\n\ndf.sentiment = df.sentiment.apply(lambda x: decode_label(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:09:35.943922Z","iopub.execute_input":"2022-02-05T12:09:35.944197Z","iopub.status.idle":"2022-02-05T12:09:36.39799Z","shell.execute_reply.started":"2022-02-05T12:09:35.944165Z","shell.execute_reply":"2022-02-05T12:09:36.397255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment.unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:10:18.262635Z","iopub.execute_input":"2022-02-05T12:10:18.262892Z","iopub.status.idle":"2022-02-05T12:10:18.383929Z","shell.execute_reply.started":"2022-02-05T12:10:18.262863Z","shell.execute_reply":"2022-02-05T12:10:18.383126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We visualize the dataset of distribution.","metadata":{}},{"cell_type":"code","source":"val_count = df.sentiment.value_counts()\nplt.figure(figsize=(8, 4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:10:39.492372Z","iopub.execute_input":"2022-02-05T12:10:39.492921Z","iopub.status.idle":"2022-02-05T12:10:39.874988Z","shell.execute_reply.started":"2022-02-05T12:10:39.492882Z","shell.execute_reply":"2022-02-05T12:10:39.874339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is without any skewness.","metadata":{}},{"cell_type":"markdown","source":"Now we explore more data.","metadata":{}},{"cell_type":"code","source":"import random\nrandom_idx_list = [random.randint(1, len(df.text)) for i in range(10)]\ndf.loc[random_idx_list, :]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:11:06.760221Z","iopub.execute_input":"2022-02-05T12:11:06.760861Z","iopub.status.idle":"2022-02-05T12:11:06.861981Z","shell.execute_reply.started":"2022-02-05T12:11:06.760823Z","shell.execute_reply":"2022-02-05T12:11:06.861228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Text values have many punctuations(문장부호) and other words. We need to get rid of (제거하다) them.","metadata":{}},{"cell_type":"markdown","source":"# Text preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Stemming / Lemmatizatoin\ngoal : reduce <<inflectional forms and sometimes derivationally (파생적으로) related forms of a word>> to a common base form.\n\n* Stemming  \nStemming usually referes to a process that chops off (chop off : 잘라내다) the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of drivational affixes (affix : 접사).\n\n* Lemmatization  \nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis (형태소 분석) of words, normally aiming to remove inflectoinal endings (굴절 어미) only and to return the base and dictoinary form of a word.","metadata":{}},{"cell_type":"markdown","source":"## Hyperlinks and Mentions\nex) @arunrk7, httpsL//keras.io","metadata":{}},{"cell_type":"markdown","source":"## Stopwords\nnltk library has functoins to perform text processing task.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n+ : 하나 이상 있을 경우\n| : or\n? : 하나 있거나, 하나도 없거나\n\"\"\"\nstemmer = SnowballStemmer('english')\n# stemmer = PorterStemmer()\ntext_cleaning_re = \"@\\S+|https?:\\S+http?:\\S|[^A-Za-z0-9]\"\nstop_words = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:12:24.533188Z","iopub.execute_input":"2022-02-05T12:12:24.533558Z","iopub.status.idle":"2022-02-05T12:12:24.543918Z","shell.execute_reply.started":"2022-02-05T12:12:24.533511Z","shell.execute_reply":"2022-02-05T12:12:24.543055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text, stem=False):\n    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:12:53.163261Z","iopub.execute_input":"2022-02-05T12:12:53.163933Z","iopub.status.idle":"2022-02-05T12:12:53.169224Z","shell.execute_reply.started":"2022-02-05T12:12:53.163896Z","shell.execute_reply":"2022-02-05T12:12:53.168321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.text = df.text.apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:12:55.932766Z","iopub.execute_input":"2022-02-05T12:12:55.933328Z","iopub.status.idle":"2022-02-05T12:13:48.761873Z","shell.execute_reply.started":"2022-02-05T12:12:55.93327Z","shell.execute_reply":"2022-02-05T12:13:48.761113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(n=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:13:48.763396Z","iopub.execute_input":"2022-02-05T12:13:48.763623Z","iopub.status.idle":"2022-02-05T12:13:48.808535Z","shell.execute_reply.started":"2022-02-05T12:13:48.76359Z","shell.execute_reply":"2022-02-05T12:13:48.807746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word cloud","metadata":{}},{"cell_type":"markdown","source":"## Positive words","metadata":{}},{"cell_type":"code","source":"df[df.sentiment=='Positive'].text","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:14:16.07165Z","iopub.execute_input":"2022-02-05T12:14:16.071901Z","iopub.status.idle":"2022-02-05T12:14:16.305351Z","shell.execute_reply.started":"2022-02-05T12:14:16.071873Z","shell.execute_reply":"2022-02-05T12:14:16.304478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nplt.figure(figsize=(20, 20))\nwc = WordCloud(max_words=2000, width=1600, height=800).generate(\" \".join(df[df.sentiment=='Positive'].text))\nplt.imshow(wc, interpolation='bilinear')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:16:45.054318Z","iopub.execute_input":"2022-02-05T12:16:45.054991Z","iopub.status.idle":"2022-02-05T12:17:45.142769Z","shell.execute_reply.started":"2022-02-05T12:16:45.054954Z","shell.execute_reply":"2022-02-05T12:17:45.142112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Negative words","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nwc = WordCloud(max_words=2000, width=1600, height=800).generate(\" \".join(df[df.sentiment=='Negative'].text))\nplt.imshow(wc, interpolation='bilinear')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:18:00.892436Z","iopub.execute_input":"2022-02-05T12:18:00.893096Z","iopub.status.idle":"2022-02-05T12:19:01.515672Z","shell.execute_reply.started":"2022-02-05T12:18:00.893058Z","shell.execute_reply":"2022-02-05T12:19:01.514314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and test split","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 30","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:20:15.983208Z","iopub.execute_input":"2022-02-05T12:20:15.983929Z","iopub.status.idle":"2022-02-05T12:20:15.987906Z","shell.execute_reply.started":"2022-02-05T12:20:15.983889Z","shell.execute_reply":"2022-02-05T12:20:15.986894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=7)\nprint('Train data size : ', len(train_data))\nprint('Test data size : ', len(test_data))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:20:16.571508Z","iopub.execute_input":"2022-02-05T12:20:16.57177Z","iopub.status.idle":"2022-02-05T12:20:16.984886Z","shell.execute_reply.started":"2022-02-05T12:20:16.571742Z","shell.execute_reply":"2022-02-05T12:20:16.984019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:20:17.040134Z","iopub.execute_input":"2022-02-05T12:20:17.040366Z","iopub.status.idle":"2022-02-05T12:20:17.048983Z","shell.execute_reply.started":"2022-02-05T12:20:17.04034Z","shell.execute_reply":"2022-02-05T12:20:17.048106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizeation\n\n'tokenizer' create tokens for every word in the data corpus and map them to a index using dictionary.  \n\n'word_index' contains the index for each word.  \n\n'vocab_size' represents the total number of word in the data corpus.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary size : ', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:21:43.604667Z","iopub.execute_input":"2022-02-05T12:21:43.604928Z","iopub.status.idle":"2022-02-05T12:22:02.203792Z","shell.execute_reply.started":"2022-02-05T12:21:43.604898Z","shell.execute_reply":"2022-02-05T12:22:02.202963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (k, v) in enumerate(word_index.items()):\n    if i == 10:\n        break\n    print(k, v)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:22:28.054191Z","iopub.execute_input":"2022-02-05T12:22:28.054893Z","iopub.status.idle":"2022-02-05T12:22:28.061592Z","shell.execute_reply.started":"2022-02-05T12:22:28.054857Z","shell.execute_reply":"2022-02-05T12:22:28.060803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we got a 'tokenizer' object, which can be used to convert any word into a Key in dictionary (number).\n\nSince we are going to build a sequence model, we should feed in a sequence of numbers to it. And also we should ensure there is no variance in input shapes of sequences. It all should be of same length. But texts in tweets have difference count of words in it. To avoid this, we seek a little help from 'pad_sequences' to do our job. It will make all the sequence in one constant length 'MAX_SEQUENCE_LENGTH'.","metadata":{}},{"cell_type":"code","source":"tokenizer.texts_to_sequences(train_data.text)[:10]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:22:50.683722Z","iopub.execute_input":"2022-02-05T12:22:50.683975Z","iopub.status.idle":"2022-02-05T12:23:07.072918Z","shell.execute_reply.started":"2022-02-05T12:22:50.683946Z","shell.execute_reply":"2022-02-05T12:23:07.072169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                       maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n                       maxlen=MAX_SEQUENCE_LENGTH)\n\nprint('Training X shape : ', x_train.shape)\nprint('Test X shape : ', x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:25:26.280934Z","iopub.execute_input":"2022-02-05T12:25:26.281196Z","iopub.status.idle":"2022-02-05T12:25:55.81989Z","shell.execute_reply.started":"2022-02-05T12:25:26.281166Z","shell.execute_reply":"2022-02-05T12:25:55.818368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train[:5]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:25:55.82153Z","iopub.execute_input":"2022-02-05T12:25:55.821859Z","iopub.status.idle":"2022-02-05T12:25:55.829873Z","shell.execute_reply.started":"2022-02-05T12:25:55.821821Z","shell.execute_reply":"2022-02-05T12:25:55.829129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.sentiment.unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:25:55.831283Z","iopub.execute_input":"2022-02-05T12:25:55.831937Z","iopub.status.idle":"2022-02-05T12:25:55.934784Z","shell.execute_reply.started":"2022-02-05T12:25:55.831901Z","shell.execute_reply":"2022-02-05T12:25:55.934123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_data.sentiment.unique().tolist()\nlabels","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:25:55.936547Z","iopub.execute_input":"2022-02-05T12:25:55.936824Z","iopub.status.idle":"2022-02-05T12:25:56.031499Z","shell.execute_reply.started":"2022-02-05T12:25:55.936787Z","shell.execute_reply":"2022-02-05T12:25:56.030724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label encoding","metadata":{}},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n\nprint('y_train shape : ', y_train.shape)\nprint('y_test shape : ', y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:26:39.790925Z","iopub.execute_input":"2022-02-05T12:26:39.791367Z","iopub.status.idle":"2022-02-05T12:26:41.248536Z","shell.execute_reply.started":"2022-02-05T12:26:39.791324Z","shell.execute_reply":"2022-02-05T12:26:41.247638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[:5]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:27:29.623615Z","iopub.execute_input":"2022-02-05T12:27:29.623871Z","iopub.status.idle":"2022-02-05T12:27:29.629046Z","shell.execute_reply.started":"2022-02-05T12:27:29.623843Z","shell.execute_reply":"2022-02-05T12:27:29.628192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word embedding","metadata":{}},{"cell_type":"markdown","source":"Using GloVe for embedding.","metadata":{}},{"cell_type":"code","source":"# !wget http://nlp.stanford.edu/data/glove.6B.zip\n# !unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:46:52.551184Z","iopub.execute_input":"2021-12-17T04:46:52.551368Z","iopub.status.idle":"2021-12-17T04:46:52.554255Z","shell.execute_reply.started":"2021-12-17T04:46:52.551345Z","shell.execute_reply":"2021-12-17T04:46:52.553594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 10\nMODEL_PATH = '.../output/kaggle/working/best_model.hdf5'","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:28:19.272406Z","iopub.execute_input":"2022-02-05T12:28:19.272693Z","iopub.status.idle":"2022-02-05T12:28:19.276917Z","shell.execute_reply.started":"2022-02-05T12:28:19.272664Z","shell.execute_reply":"2022-02-05T12:28:19.276255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings_index = {}\n\n# f = open(GLOVE_EMB)\n# for line in f:\n#     values = line.split()\n#     word = value = values[0]\n#     coefs = np.asarray(values[1:], dtype='float32')\n#     embeddings_index[word] = coefs\n# f.close()\n\n# print('Found %s word vectors' %len(embeddings_index))\n# # for i, (k, v) in enumerate(embeddings_index.items()):\n# #     if i == 5:\n# #         break\n# #     print(k, v)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:46:52.563672Z","iopub.execute_input":"2021-12-17T04:46:52.564009Z","iopub.status.idle":"2021-12-17T04:46:52.573638Z","shell.execute_reply.started":"2021-12-17T04:46:52.563973Z","shell.execute_reply":"2021-12-17T04:46:52.572858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n# for word, i in word_index.items():\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None:\n#         embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:46:52.574891Z","iopub.execute_input":"2021-12-17T04:46:52.575668Z","iopub.status.idle":"2021-12-17T04:46:52.581701Z","shell.execute_reply.started":"2021-12-17T04:46:52.575632Z","shell.execute_reply":"2021-12-17T04:46:52.581003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding_layer = tf.keras.layers.Embedding(vocab_size,\n#                                            EMBEDDING_DIM,\n#                                            weights=[embedding_matrix],\n#                                            input_length=MAX_SEQUENCE_LENGTH,\n#                                            trainable=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:46:52.583662Z","iopub.execute_input":"2021-12-17T04:46:52.584224Z","iopub.status.idle":"2021-12-17T04:46:52.590225Z","shell.execute_reply.started":"2021-12-17T04:46:52.584187Z","shell.execute_reply":"2021-12-17T04:46:52.589505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using word2vec for embedding.","metadata":{}},{"cell_type":"code","source":"import gensim\nfrom urllib.request import urlretrieve","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:30:15.403697Z","iopub.execute_input":"2022-02-05T12:30:15.403966Z","iopub.status.idle":"2022-02-05T12:30:15.525248Z","shell.execute_reply.started":"2022-02-05T12:30:15.403936Z","shell.execute_reply":"2022-02-05T12:30:15.524506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \n            filename=\"/kaggle/working/GoogleNews-vectors-negative300.bin.gz\")\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/kaggle/working/GoogleNews-vectors-negative300.bin.gz', binary=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:32:52.269982Z","iopub.execute_input":"2022-02-05T12:32:52.270492Z","iopub.status.idle":"2022-02-05T12:34:44.247757Z","shell.execute_reply.started":"2022-02-05T12:32:52.270452Z","shell.execute_reply":"2022-02-05T12:34:44.246852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec_model.vectors.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:34:44.253109Z","iopub.execute_input":"2022-02-05T12:34:44.255751Z","iopub.status.idle":"2022-02-05T12:34:44.264951Z","shell.execute_reply.started":"2022-02-05T12:34:44.255711Z","shell.execute_reply":"2022-02-05T12:34:44.264172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_vector(word):\n    if word in word2vec_model:\n        return word2vec_model[word]\n    else:\n        return None","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:34:44.269689Z","iopub.execute_input":"2022-02-05T12:34:44.270236Z","iopub.status.idle":"2022-02-05T12:34:44.278175Z","shell.execute_reply.started":"2022-02-05T12:34:44.2702Z","shell.execute_reply":"2022-02-05T12:34:44.277341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec_model.vectors[:10]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:34:44.279968Z","iopub.execute_input":"2022-02-05T12:34:44.280854Z","iopub.status.idle":"2022-02-05T12:34:44.296656Z","shell.execute_reply.started":"2022-02-05T12:34:44.280818Z","shell.execute_reply":"2022-02-05T12:34:44.295329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = get_vector(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:49:36.552681Z","iopub.execute_input":"2022-02-05T12:49:36.552943Z","iopub.status.idle":"2022-02-05T12:49:37.374129Z","shell.execute_reply.started":"2022-02-05T12:49:36.552914Z","shell.execute_reply":"2022-02-05T12:49:37.373325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                           EMBEDDING_DIM,\n                                           weights=[embedding_matrix],\n                                           input_length=MAX_SEQUENCE_LENGTH,\n                                           trainable=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:49:37.375665Z","iopub.execute_input":"2022-02-05T12:49:37.376058Z","iopub.status.idle":"2022-02-05T12:49:37.979535Z","shell.execute_reply.started":"2022-02-05T12:49:37.376012Z","shell.execute_reply":"2022-02-05T12:49:37.978738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training - LSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:49:40.602605Z","iopub.execute_input":"2022-02-05T12:49:40.603292Z","iopub.status.idle":"2022-02-05T12:49:40.61412Z","shell.execute_reply.started":"2022-02-05T12:49:40.603257Z","shell.execute_reply":"2022-02-05T12:49:40.613354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:50:14.116593Z","iopub.execute_input":"2022-02-05T12:50:14.117361Z","iopub.status.idle":"2022-02-05T12:50:17.302937Z","shell.execute_reply.started":"2022-02-05T12:50:14.117299Z","shell.execute_reply":"2022-02-05T12:50:17.302255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T12:55:00.544796Z","iopub.execute_input":"2022-02-05T12:55:00.545083Z","iopub.status.idle":"2022-02-05T12:55:00.887972Z","shell.execute_reply.started":"2022-02-05T12:55:00.545049Z","shell.execute_reply":"2022-02-05T12:55:00.887197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* LRScheduler  \nIt changes a learning rate at specific epoch to achieve more improved result. In this notebook, the learning rate exponentionally decreases after remaining same for first 10 Epoch.\n\n* ModelCheckPoint  \nIt saves best model while training based on some metrics. Here, it saves the model with minimum Validity Loss.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr=0.01,\n                                     monitor='val_loss',\n                                     verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:48:31.364262Z","iopub.execute_input":"2021-12-17T04:48:31.364909Z","iopub.status.idle":"2021-12-17T04:48:31.384534Z","shell.execute_reply.started":"2021-12-17T04:48:31.364865Z","shell.execute_reply":"2021-12-17T04:48:31.383849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:48:31.385887Z","iopub.execute_input":"2021-12-17T04:48:31.386172Z","iopub.status.idle":"2021-12-17T04:48:31.39794Z","shell.execute_reply.started":"2021-12-17T04:48:31.386137Z","shell.execute_reply":"2021-12-17T04:48:31.397049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, \n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS, \n                    validation_data=(x_test, y_test), \n                    callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:48:31.39953Z","iopub.execute_input":"2021-12-17T04:48:31.400016Z","iopub.status.idle":"2021-12-17T05:10:56.938766Z","shell.execute_reply.started":"2021-12-17T04:48:31.399979Z","shell.execute_reply":"2021-12-17T05:10:56.938015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation","metadata":{}},{"cell_type":"code","source":"s, (at, al) = plt.subplots(2, 1)\nprint(s)\nprint(at, al)\nat.plot(history.history['accuracy'], c='b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc='upper left')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:10:56.940586Z","iopub.execute_input":"2021-12-17T05:10:56.940867Z","iopub.status.idle":"2021-12-17T05:10:57.253577Z","shell.execute_reply.started":"2021-12-17T05:10:56.940832Z","shell.execute_reply":"2021-12-17T05:10:57.252898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_sentiment(score):\n    return 'Positive' if score > 0.5 else 'Negative'\n\nscores = model.predict(x_test, verbose=1, batch_size=10000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:10:57.254693Z","iopub.execute_input":"2021-12-17T05:10:57.257132Z","iopub.status.idle":"2021-12-17T05:11:00.830944Z","shell.execute_reply.started":"2021-12-17T05:10:57.257089Z","shell.execute_reply":"2021-12-17T05:11:00.830151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_1d[:10]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:11:00.832451Z","iopub.execute_input":"2021-12-17T05:11:00.832712Z","iopub.status.idle":"2021-12-17T05:11:00.840601Z","shell.execute_reply.started":"2021-12-17T05:11:00.832677Z","shell.execute_reply":"2021-12-17T05:11:00.839246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(list(test_data.sentiment), y_pred_1d))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:11:00.842773Z","iopub.execute_input":"2021-12-17T05:11:00.843072Z","iopub.status.idle":"2021-12-17T05:11:05.128825Z","shell.execute_reply.started":"2021-12-17T05:11:00.843043Z","shell.execute_reply":"2021-12-17T05:11:05.12804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}